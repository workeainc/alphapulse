#!/usr/bin/env python3
"""
Phase 5B: Enhanced Ensemble Manager with Regime-Aware Meta-Learner
Implements:
1. Multiple model ensemble (GBM, LightGBM, XGBoost, Transformer, LSTM)
2. Regime-aware meta-learner for model selection
3. Enhanced performance through model diversity
4. Regime-aware model switching
"""

import asyncio
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from pathlib import Path
import json
import pickle
import time
from dataclasses import dataclass, field
from enum import Enum
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold
import joblib

# Deep Learning Libraries
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("PyTorch not available for Transformer/LSTM models")

# ML Libraries
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# Local imports
from ..model_registry import ModelRegistry
from ..advanced_logging_system import redis_logger, EventType, LogLevel
from ...database.connection import TimescaleDBConnection
try:
    from ..features.feature_store import feature_store
except ImportError:
    from ..features.feature_store_simple import simple_feature_store as feature_store

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Enhanced model types for Phase 5B"""
    XGBOOST = "xgboost"
    LIGHTGBM = "lightgbm"
    GRADIENT_BOOSTING = "gradient_boosting"
    RANDOM_FOREST = "random_forest"
    TRANSFORMER = "transformer"
    LSTM = "lstm"
    LOGISTIC_REGRESSION = "logistic_regression"

class MarketRegime(Enum):
    """Market regime types"""
    BULL_TRENDING = "bull_trending"
    BEAR_TRENDING = "bear_trending"
    SIDEWAYS = "sideways"
    HIGH_VOLATILITY = "high_volatility"
    LOW_VOLATILITY = "low_volatility"
    CRASH = "crash"

@dataclass
class RegimeFeatures:
    """Features for regime detection"""
    volatility: float
    trend_strength: float
    btc_dominance: float
    market_correlation: float
    volume_ratio: float
    atr_percentage: float
    regime: MarketRegime = MarketRegime.SIDEWAYS

@dataclass
class ModelPerformance:
    """Model performance metrics per regime"""
    model_type: ModelType
    regime: MarketRegime
    auc: float
    accuracy: float
    precision: float
    recall: float
    f1: float
    sample_count: int
    last_updated: datetime
    confidence: float = 0.0

@dataclass
class EnsembleConfig:
    """Enhanced ensemble configuration for Phase 5B"""
    # Model types to include
    model_types: List[ModelType] = field(default_factory=lambda: [
        ModelType.XGBOOST, ModelType.LIGHTGBM, ModelType.GRADIENT_BOOSTING,
        ModelType.RANDOM_FOREST, ModelType.TRANSFORMER, ModelType.LSTM
    ])
    
    # Meta-learner configuration
    meta_learner_type: str = "regime_aware_logistic"
    meta_learner_params: Dict[str, Any] = field(default_factory=lambda: {
        'C': 1.0, 'max_iter': 1000, 'random_state': 42
    })
    
    # Regime-specific weights
    regime_weights: Dict[MarketRegime, Dict[str, float]] = field(default_factory=lambda: {
        MarketRegime.BULL_TRENDING: {
            'xgboost': 0.25, 'lightgbm': 0.25, 'gradient_boosting': 0.2,
            'random_forest': 0.1, 'transformer': 0.1, 'lstm': 0.1
        },
        MarketRegime.BEAR_TRENDING: {
            'xgboost': 0.2, 'lightgbm': 0.2, 'gradient_boosting': 0.2,
            'random_forest': 0.15, 'transformer': 0.15, 'lstm': 0.1
        },
        MarketRegime.SIDEWAYS: {
            'xgboost': 0.15, 'lightgbm': 0.15, 'gradient_boosting': 0.15,
            'random_forest': 0.2, 'transformer': 0.2, 'lstm': 0.15
        },
        MarketRegime.HIGH_VOLATILITY: {
            'xgboost': 0.1, 'lightgbm': 0.1, 'gradient_boosting': 0.1,
            'random_forest': 0.25, 'transformer': 0.25, 'lstm': 0.2
        },
        MarketRegime.LOW_VOLATILITY: {
            'xgboost': 0.3, 'lightgbm': 0.3, 'gradient_boosting': 0.2,
            'random_forest': 0.1, 'transformer': 0.05, 'lstm': 0.05
        },
        MarketRegime.CRASH: {
            'xgboost': 0.05, 'lightgbm': 0.05, 'gradient_boosting': 0.1,
            'random_forest': 0.3, 'transformer': 0.3, 'lstm': 0.2
        }
    })
    
    # Training parameters
    cv_folds: int = 5
    validation_split: float = 0.2
    random_state: int = 42
    
    # Regime detection parameters
    regime_detection_window: int = 100
    regime_confidence_threshold: float = 0.7

@dataclass
class EnsemblePrediction:
    """Enhanced ensemble prediction result"""
    individual_predictions: Dict[str, float]
    ensemble_prediction: float
    confidence: float
    selected_models: List[str]
    regime: MarketRegime
    regime_confidence: float
    model_weights: Dict[str, float]
    meta_learner_score: float
    timestamp: datetime = field(default_factory=datetime.now)

class TransformerModel(nn.Module):
    """Simple Transformer model for sequence prediction"""
    
    def __init__(self, input_size: int, d_model: int = 64, nhead: int = 4, 
                 num_layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Linear(input_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, 100, d_model))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        x = self.embedding(x)
        x = x + self.pos_encoding[:, :x.size(1), :]
        x = self.transformer(x)
        x = x.mean(dim=1)  # Global average pooling
        return self.classifier(x)
    
    def predict(self, x):
        """Predict method for compatibility with sklearn interface"""
        self.eval()
        with torch.no_grad():
            if isinstance(x, np.ndarray):
                x = torch.FloatTensor(x)
            if len(x.shape) == 2:
                x = x.unsqueeze(1)  # Add sequence dimension
            outputs = self.forward(x)
            return outputs.squeeze().cpu().numpy()
    
    def predict_proba(self, x):
        """Predict probabilities for compatibility with sklearn interface"""
        preds = self.predict(x)
        # Convert to 2D array for sklearn compatibility
        if len(preds.shape) == 1:
            preds = preds.reshape(-1, 1)
        return np.column_stack([1 - preds, preds])
    
    def fit(self, X, y):
        """Fit method for compatibility with sklearn interface"""
        # This is a simplified fit for evaluation purposes
        # In practice, the model should be trained properly
        return self

class LSTMModel(nn.Module):
    """LSTM model for sequence prediction"""
    
    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 2, 
                 dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]  # Take last output
        return self.classifier(lstm_out)
    
    def predict(self, x):
        """Predict method for compatibility with sklearn interface"""
        self.eval()
        with torch.no_grad():
            if isinstance(x, np.ndarray):
                x = torch.FloatTensor(x)
            if len(x.shape) == 2:
                x = x.unsqueeze(1)  # Add sequence dimension
            outputs = self.forward(x)
            return outputs.squeeze().cpu().numpy()
    
    def predict_proba(self, x):
        """Predict probabilities for compatibility with sklearn interface"""
        preds = self.predict(x)
        # Convert to 2D array for sklearn compatibility
        if len(preds.shape) == 1:
            preds = preds.reshape(-1, 1)
        return np.column_stack([1 - preds, preds])
    
    def fit(self, X, y):
        """Fit method for compatibility with sklearn interface"""
        # This is a simplified fit for evaluation purposes
        # In practice, the model should be trained properly
        return self

class RegimeAwareMetaLearner:
    """Meta-learner that considers market regime for model selection"""
    
    def __init__(self, config: EnsembleConfig):
        self.config = config
        self.regime_classifier = LogisticRegression(**config.meta_learner_params)
        self.model_performance = {}  # Dict[ModelType, Dict[MarketRegime, ModelPerformance]]
        self.regime_weights = config.regime_weights
        self.logger = logger
    
    def detect_regime(self, features: pd.DataFrame) -> Tuple[MarketRegime, float]:
        """Detect current market regime"""
        try:
            # Extract regime features
            regime_features = self._extract_regime_features(features)
            
            # Simple rule-based regime detection (can be enhanced with ML)
            volatility = regime_features.volatility
            trend_strength = regime_features.trend_strength
            btc_dominance = regime_features.btc_dominance
            
            # Regime classification logic
            if volatility > 0.05:  # High volatility
                if trend_strength < 0.1:
                    regime = MarketRegime.HIGH_VOLATILITY
                else:
                    regime = MarketRegime.CRASH
            elif trend_strength > 0.3:  # Strong trend
                if btc_dominance > 0.5:
                    regime = MarketRegime.BULL_TRENDING
                else:
                    regime = MarketRegime.BEAR_TRENDING
            elif volatility < 0.02:  # Low volatility
                regime = MarketRegime.LOW_VOLATILITY
            else:
                regime = MarketRegime.SIDEWAYS
            
            confidence = min(0.9, 0.5 + abs(trend_strength) + abs(volatility - 0.03))
            
            return regime, confidence
            
        except Exception as e:
            self.logger.error(f"Error detecting regime: {e}")
            return MarketRegime.SIDEWAYS, 0.5
    
    def _extract_regime_features(self, features: pd.DataFrame) -> RegimeFeatures:
        """Extract features for regime detection"""
        try:
            # Calculate basic regime features
            returns = features['close'].pct_change().dropna()
            volatility = returns.std()
            trend_strength = abs(returns.mean()) / returns.std() if returns.std() > 0 else 0
            
            # Extract other features (assuming they exist in the dataframe)
            btc_dominance = features.get('btc_dominance', 0.5).iloc[-1] if 'btc_dominance' in features else 0.5
            market_correlation = features.get('market_correlation', 0.0).iloc[-1] if 'market_correlation' in features else 0.0
            volume_ratio = features.get('volume_ratio', 1.0).iloc[-1] if 'volume_ratio' in features else 1.0
            atr_percentage = features.get('atr_percentage', 0.02).iloc[-1] if 'atr_percentage' in features else 0.02
            
            return RegimeFeatures(
                volatility=volatility,
                trend_strength=trend_strength,
                btc_dominance=btc_dominance,
                market_correlation=market_correlation,
                volume_ratio=volume_ratio,
                atr_percentage=atr_percentage
            )
            
        except Exception as e:
            self.logger.error(f"Error extracting regime features: {e}")
            return RegimeFeatures(
                volatility=0.02, trend_strength=0.0, btc_dominance=0.5,
                market_correlation=0.0, volume_ratio=1.0, atr_percentage=0.02
            )
    
    def get_regime_weights(self, regime: MarketRegime) -> Dict[str, float]:
        """Get model weights for specific regime"""
        return self.regime_weights.get(regime, self.regime_weights[MarketRegime.SIDEWAYS])
    
    def update_model_performance(self, model_type: ModelType, regime: MarketRegime, 
                               performance: ModelPerformance):
        """Update model performance for specific regime"""
        if model_type not in self.model_performance:
            self.model_performance[model_type] = {}
        
        self.model_performance[model_type][regime] = performance
    
    def select_best_models(self, regime: MarketRegime, top_k: int = 3) -> List[ModelType]:
        """Select best performing models for current regime"""
        try:
            regime_performances = []
            
            for model_type, regime_perfs in self.model_performance.items():
                if regime in regime_perfs:
                    perf = regime_perfs[regime]
                    regime_performances.append((model_type, perf.auc))
            
            # Sort by AUC and return top k
            regime_performances.sort(key=lambda x: x[1], reverse=True)
            return [model_type for model_type, _ in regime_performances[:top_k]]
            
        except Exception as e:
            self.logger.error(f"Error selecting best models: {e}")
            return [ModelType.XGBOOST, ModelType.LIGHTGBM, ModelType.GRADIENT_BOOSTING]

class EnhancedEnsembleManager:
    """Phase 5B: Enhanced Ensemble Manager with regime-aware meta-learner"""
    
    def __init__(self, config: EnsembleConfig = None):
        self.config = config or EnsembleConfig()
        self.logger = logger
        
        # Initialize models
        self.models = {}
        self.meta_learner = RegimeAwareMetaLearner(self.config)
        
        # Database connection
        self.db_connection = TimescaleDBConnection()
        
        # Model registry
        self.model_registry = ModelRegistry()
        
        # Feature store integration
        self.feature_store = feature_store
        
        # Performance tracking
        self.performance_history = []
        
        # Initialize models
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize all model types"""
        try:
            # Traditional ML models
            if XGBOOST_AVAILABLE:
                self.models[ModelType.XGBOOST] = None
            
            if LIGHTGBM_AVAILABLE:
                self.models[ModelType.LIGHTGBM] = None
            
            self.models[ModelType.GRADIENT_BOOSTING] = None
            self.models[ModelType.RANDOM_FOREST] = None
            self.models[ModelType.LOGISTIC_REGRESSION] = None
            
            # Deep Learning models
            if TORCH_AVAILABLE:
                self.models[ModelType.TRANSFORMER] = None
                self.models[ModelType.LSTM] = None
            
            self.logger.info(f"‚úÖ Initialized {len(self.models)} model types")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error initializing models: {e}")
    
    async def train_model(self, model_type: ModelType, X: pd.DataFrame, y: pd.Series,
                         regime: MarketRegime = None) -> bool:
        """Train a specific model type"""
        try:
            self.logger.info(f"üîÑ Training {model_type.value} model...")
            
            if model_type == ModelType.XGBOOST and XGBOOST_AVAILABLE:
                model = await self._train_xgboost(X, y)
            elif model_type == ModelType.LIGHTGBM and LIGHTGBM_AVAILABLE:
                model = await self._train_lightgbm(X, y)
            elif model_type == ModelType.GRADIENT_BOOSTING:
                model = await self._train_gradient_boosting(X, y)
            elif model_type == ModelType.RANDOM_FOREST:
                model = await self._train_random_forest(X, y)
            elif model_type == ModelType.LOGISTIC_REGRESSION:
                model = await self._train_logistic_regression(X, y)
            elif model_type == ModelType.TRANSFORMER and TORCH_AVAILABLE:
                model = await self._train_transformer(X, y)
            elif model_type == ModelType.LSTM and TORCH_AVAILABLE:
                model = await self._train_lstm(X, y)
            else:
                self.logger.warning(f"‚ö†Ô∏è Model type {model_type.value} not available")
                return False
            
            if model is not None:
                self.models[model_type] = model
                
                # Evaluate and store performance
                if regime:
                    performance = await self._evaluate_model(model, model_type, X, y, regime)
                    self.meta_learner.update_model_performance(model_type, regime, performance)
                
                # Save model
                await self._save_model(model, model_type)
                
                self.logger.info(f"‚úÖ {model_type.value} model trained successfully")
                return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"‚ùå Error training {model_type.value} model: {e}")
            return False
    
    async def _train_xgboost(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train XGBoost model"""
        try:
            model = xgb.XGBClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=self.config.random_state,
                eval_metric='logloss'
            )
            model.fit(X, y)
            return model
        except Exception as e:
            self.logger.error(f"Error training XGBoost: {e}")
            return None
    
    async def _train_lightgbm(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train LightGBM model"""
        try:
            model = lgb.LGBMClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=self.config.random_state,
                verbose=-1
            )
            model.fit(X, y)
            return model
        except Exception as e:
            self.logger.error(f"Error training LightGBM: {e}")
            return None
    
    async def _train_gradient_boosting(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train Gradient Boosting model"""
        try:
            model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=self.config.random_state
            )
            model.fit(X, y)
            return model
        except Exception as e:
            self.logger.error(f"Error training Gradient Boosting: {e}")
            return None
    
    async def _train_random_forest(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train Random Forest model"""
        try:
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=self.config.random_state
            )
            model.fit(X, y)
            return model
        except Exception as e:
            self.logger.error(f"Error training Random Forest: {e}")
            return None
    
    async def _train_logistic_regression(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train Logistic Regression model"""
        try:
            model = LogisticRegression(
                random_state=self.config.random_state,
                max_iter=1000
            )
            model.fit(X, y)
            return model
        except Exception as e:
            self.logger.error(f"Error training Logistic Regression: {e}")
            return None
    
    async def _train_transformer(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train Transformer model"""
        try:
            # Prepare sequence data
            X_tensor = torch.FloatTensor(X.values)
            y_tensor = torch.FloatTensor(y.values)
            
            # Reshape for sequence (batch_size, seq_len, features)
            if len(X_tensor.shape) == 2:
                X_tensor = X_tensor.unsqueeze(1)  # Add sequence dimension
            
            # Create model
            input_size = X_tensor.shape[-1]
            model = TransformerModel(input_size=input_size)
            
            # Training setup
            criterion = nn.BCELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # Simple training loop
            model.train()
            for epoch in range(10):
                optimizer.zero_grad()
                outputs = model(X_tensor).squeeze()
                loss = criterion(outputs, y_tensor)
                loss.backward()
                optimizer.step()
            
            return model
            
        except Exception as e:
            self.logger.error(f"Error training Transformer: {e}")
            return None
    
    async def _train_lstm(self, X: pd.DataFrame, y: pd.Series) -> Any:
        """Train LSTM model"""
        try:
            # Prepare sequence data
            X_tensor = torch.FloatTensor(X.values)
            y_tensor = torch.FloatTensor(y.values)
            
            # Reshape for sequence (batch_size, seq_len, features)
            if len(X_tensor.shape) == 2:
                X_tensor = X_tensor.unsqueeze(1)  # Add sequence dimension
            
            # Create model
            input_size = X_tensor.shape[-1]
            model = LSTMModel(input_size=input_size)
            
            # Training setup
            criterion = nn.BCELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # Simple training loop
            model.train()
            for epoch in range(10):
                optimizer.zero_grad()
                outputs = model(X_tensor).squeeze()
                loss = criterion(outputs, y_tensor)
                loss.backward()
                optimizer.step()
            
            return model
            
        except Exception as e:
            self.logger.error(f"Error training LSTM: {e}")
            return None
    
    async def _evaluate_model(self, model: Any, model_type: ModelType, 
                            X: pd.DataFrame, y: pd.Series, regime: MarketRegime) -> ModelPerformance:
        """Evaluate model performance"""
        try:
            # Cross-validation
            cv_scores = []
            skf = StratifiedKFold(n_splits=self.config.cv_folds, shuffle=True, 
                                random_state=self.config.random_state)
            
            for train_idx, val_idx in skf.split(X, y):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # Train on fold
                model_copy = self._clone_model(model)
                if hasattr(model_copy, 'fit'):
                    # Convert data format for deep learning models
                    if model_type in [ModelType.TRANSFORMER, ModelType.LSTM]:
                        X_train_tensor = torch.FloatTensor(X_train.values)
                        y_train_tensor = torch.FloatTensor(y_train.values)
                        if len(X_train_tensor.shape) == 2:
                            X_train_tensor = X_train_tensor.unsqueeze(1)
                        model_copy.fit(X_train_tensor, y_train_tensor)
                    else:
                        model_copy.fit(X_train, y_train)
                
                # Predict
                if model_type in [ModelType.TRANSFORMER, ModelType.LSTM]:
                    # Convert validation data to tensor
                    X_val_tensor = torch.FloatTensor(X_val.values)
                    if len(X_val_tensor.shape) == 2:
                        X_val_tensor = X_val_tensor.unsqueeze(1)
                    
                    if hasattr(model_copy, 'predict_proba'):
                        y_pred_proba = model_copy.predict_proba(X_val_tensor)[:, 1]
                    else:
                        y_pred_proba = model_copy.predict(X_val_tensor)
                else:
                    if hasattr(model_copy, 'predict_proba'):
                        y_pred_proba = model_copy.predict_proba(X_val)[:, 1]
                    else:
                        y_pred_proba = model_copy.predict(X_val)
                
                # Calculate metrics
                auc = roc_auc_score(y_val, y_pred_proba)
                cv_scores.append(auc)
            
            # Average metrics
            avg_auc = np.mean(cv_scores)
            
            # Final prediction on full dataset
            if model_type in [ModelType.TRANSFORMER, ModelType.LSTM]:
                X_tensor = torch.FloatTensor(X.values)
                if len(X_tensor.shape) == 2:
                    X_tensor = X_tensor.unsqueeze(1)
                
                if hasattr(model, 'predict_proba'):
                    y_pred_proba = model.predict_proba(X_tensor)[:, 1]
                else:
                    y_pred_proba = model.predict(X_tensor)
            else:
                if hasattr(model, 'predict_proba'):
                    y_pred_proba = model.predict_proba(X)[:, 1]
                else:
                    y_pred_proba = model.predict(X)
            
            y_pred = (y_pred_proba > 0.5).astype(int)
            
            return ModelPerformance(
                model_type=model_type,
                regime=regime,
                auc=avg_auc,
                accuracy=accuracy_score(y, y_pred),
                precision=precision_score(y, y_pred, zero_division=0),
                recall=recall_score(y, y_pred, zero_division=0),
                f1=f1_score(y, y_pred, zero_division=0),
                sample_count=len(y),
                last_updated=datetime.now(),
                confidence=avg_auc
            )
            
        except Exception as e:
            self.logger.error(f"Error evaluating model: {e}")
            return ModelPerformance(
                model_type=model_type,
                regime=regime,
                auc=0.5, accuracy=0.5, precision=0.5, recall=0.5, f1=0.5,
                sample_count=len(y), last_updated=datetime.now(), confidence=0.0
            )
    
    def _clone_model(self, model: Any) -> Any:
        """Clone a model for cross-validation"""
        try:
            if hasattr(model, 'copy'):
                return model.copy()
            elif hasattr(model, 'clone'):
                return model.clone()
            elif hasattr(model, 'get_params'):
                # For sklearn models, create new instance with same parameters
                model_class = type(model)
                return model_class(**model.get_params())
            else:
                # For deep learning models or other models without get_params
                # Create a new instance of the same class
                model_class = type(model)
                if model_class == TransformerModel:
                    return TransformerModel(input_size=model.d_model)
                elif model_class == LSTMModel:
                    return LSTMModel(input_size=model.lstm.input_size)
                else:
                    # For unknown models, return the original
                    return model
        except Exception as e:
            self.logger.error(f"Error cloning model: {e}")
            return model
    
    async def _save_model(self, model: Any, model_type: ModelType):
        """Save model to file system"""
        try:
            model_path = f"models/phase5b_{model_type.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            
            # Save model
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            
            # Also save with a simple name for testing
            simple_path = f"models/{model_type.value}_model.pkl"
            with open(simple_path, 'wb') as f:
                pickle.dump(model, f)
            
            self.logger.info(f"‚úÖ Saved {model_type.value} model to {model_path}")
            
        except Exception as e:
            self.logger.error(f"Error saving model: {e}")
    
    async def predict(self, X: pd.DataFrame) -> EnsemblePrediction:
        """Make ensemble prediction with regime-aware model selection"""
        try:
            # Detect current regime
            regime, regime_confidence = self.meta_learner.detect_regime(X)
            
            # Get individual predictions
            individual_predictions = {}
            available_models = []
            
            for model_type, model in self.models.items():
                if model is not None:
                    try:
                        # Convert DataFrame to appropriate format for each model type
                        if model_type in [ModelType.TRANSFORMER, ModelType.LSTM]:
                            # For deep learning models, convert to tensor
                            X_tensor = torch.FloatTensor(X.values)
                            if len(X_tensor.shape) == 2:
                                X_tensor = X_tensor.unsqueeze(1)  # Add sequence dimension
                            
                            if hasattr(model, 'predict_proba'):
                                pred = model.predict_proba(X_tensor)[:, 1]
                            else:
                                pred = model.predict(X_tensor)
                        else:
                            # For traditional ML models, use DataFrame directly
                            if hasattr(model, 'predict_proba'):
                                pred = model.predict_proba(X)[:, 1]
                            else:
                                pred = model.predict(X)
                        
                        individual_predictions[model_type.value] = float(pred[0])
                        available_models.append(model_type)
                        
                    except Exception as e:
                        self.logger.warning(f"Error getting prediction from {model_type.value}: {e}")
            
            if not individual_predictions:
                raise ValueError("No model predictions available")
            
            # Select best models for current regime
            best_models = self.meta_learner.select_best_models(regime, top_k=3)
            available_best_models = [m for m in best_models if m in available_models]
            
            if not available_best_models:
                available_best_models = list(available_models)[:3]
            
            # Get regime-specific weights
            regime_weights = self.meta_learner.get_regime_weights(regime)
            
            # Calculate weighted ensemble prediction
            weighted_sum = 0.0
            total_weight = 0.0
            
            for model_type in available_best_models:
                if model_type.value in individual_predictions:
                    weight = regime_weights.get(model_type.value, 1.0 / len(available_best_models))
                    weighted_sum += individual_predictions[model_type.value] * weight
                    total_weight += weight
            
            ensemble_prediction = weighted_sum / total_weight if total_weight > 0 else 0.5
            
            # Calculate confidence
            confidence = abs(ensemble_prediction - 0.5) * 2
            
            # Meta-learner score (regime confidence)
            meta_learner_score = regime_confidence
            
            return EnsemblePrediction(
                individual_predictions=individual_predictions,
                ensemble_prediction=ensemble_prediction,
                confidence=confidence,
                selected_models=[m.value for m in available_best_models],
                regime=regime,
                regime_confidence=regime_confidence,
                model_weights=regime_weights,
                meta_learner_score=meta_learner_score
            )
            
        except Exception as e:
            self.logger.error(f"Error making ensemble prediction: {e}")
            return EnsemblePrediction(
                individual_predictions={},
                ensemble_prediction=0.5,
                confidence=0.0,
                selected_models=[],
                regime=MarketRegime.SIDEWAYS,
                regime_confidence=0.0,
                model_weights={},
                meta_learner_score=0.0
            )
    
    async def train_all_models(self, X: pd.DataFrame, y: pd.Series) -> Dict[ModelType, bool]:
        """Train all available models"""
        try:
            self.logger.info("üîÑ Training all ensemble models...")
            
            # Detect regime for performance tracking
            regime, _ = self.meta_learner.detect_regime(X)
            
            results = {}
            for model_type in self.config.model_types:
                if model_type in self.models:
                    success = await self.train_model(model_type, X, y, regime)
                    results[model_type] = success
            
            self.logger.info(f"‚úÖ Training completed. Results: {results}")
            return results
            
        except Exception as e:
            self.logger.error(f"Error training all models: {e}")
            return {}
    
    async def get_ensemble_status(self) -> Dict[str, Any]:
        """Get current ensemble status"""
        try:
            status = {
                'models_trained': {},
                'meta_learner': {
                    'regime_weights': self.meta_learner.regime_weights,
                    'model_performance': {}
                },
                'config': {
                    'model_types': [mt.value for mt in self.config.model_types],
                    'meta_learner_type': self.config.meta_learner_type
                },
                'feature_store': {
                    'integrated': True,
                    'contracts_available': []
                }
            }
            
            # Model status
            for model_type, model in self.models.items():
                status['models_trained'][model_type.value] = model is not None
            
            # Performance status
            for model_type, regime_perfs in self.meta_learner.model_performance.items():
                status['meta_learner']['model_performance'][model_type.value] = {
                    regime.value: {
                        'auc': perf.auc,
                        'accuracy': perf.accuracy,
                        'confidence': perf.confidence
                    } for regime, perf in regime_perfs.items()
                }
            
            # Feature store status
            try:
                contract = await self.feature_store.get_feature_contract('phase5b_ensemble_features')
                if contract:
                    status['feature_store']['contracts_available'].append(contract.name)
            except Exception as e:
                self.logger.warning(f"Error getting feature store status: {e}")
            
            return status
            
        except Exception as e:
            self.logger.error(f"Error getting ensemble status: {e}")
            return {}
    
    async def get_features_with_validation(self, timestamp: datetime = None) -> pd.DataFrame:
        """Get features from feature store with validation"""
        try:
            self.logger.info("üîç Getting features from feature store with validation...")
            
            # Get features from feature store
            features_df = await self.feature_store.get_features_for_ensemble(timestamp)
            
            if features_df.empty:
                self.logger.warning("‚ö†Ô∏è No features retrieved from feature store")
                return pd.DataFrame()
            
            # Validate features
            features_dict = features_df.iloc[0].to_dict()
            is_valid, errors = await self.feature_store.validate_features(
                features_dict, 'phase5b_ensemble_features'
            )
            
            if not is_valid:
                self.logger.error(f"‚ùå Feature validation failed: {errors}")
                return pd.DataFrame()
            
            self.logger.info(f"‚úÖ Features validated successfully: {list(features_df.columns)}")
            return features_df
            
        except Exception as e:
            self.logger.error(f"Error getting features with validation: {e}")
            return pd.DataFrame()
    
    async def detect_feature_drift(self, feature_names: List[str] = None) -> Dict[str, Any]:
        """Detect drift in features"""
        try:
            self.logger.info("üîç Detecting feature drift...")
            
            if not feature_names:
                # Get default Phase 5B features
                contract = await self.feature_store.get_feature_contract('phase5b_ensemble_features')
                if contract:
                    feature_names = contract.schema_contract.get('required_features', [])
                else:
                    feature_names = ['close_price', 'volume', 'btc_dominance', 'market_correlation', 'volume_ratio', 'atr_percentage']
            
            drift_results = {}
            for feature_name in feature_names:
                drift_result = await self.feature_store.detect_drift(feature_name)
                if drift_result:
                    drift_results[feature_name] = {
                        'drift_type': drift_result.drift_type.value,
                        'drift_score': drift_result.drift_score,
                        'is_drift_detected': drift_result.is_drift_detected,
                        'threshold': drift_result.threshold
                    }
            
            self.logger.info(f"‚úÖ Drift detection completed: {len(drift_results)} features checked")
            return drift_results
            
        except Exception as e:
            self.logger.error(f"Error detecting feature drift: {e}")
            return {}

# Global instance
enhanced_ensemble_manager = EnhancedEnsembleManager()
